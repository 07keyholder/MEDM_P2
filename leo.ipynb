{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import gower\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./in-vehicle-coupon-recommendation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['car'])\n",
    "df = df.drop(columns=['toCoupon_GEQ5min'])\n",
    "df = df.drop(columns=['direction_opp'])\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column].fillna(df[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_mapping(df: pd.DataFrame):\n",
    "    frequency_map = {'never': 0,'less1': 1,'1~3': 2,'4~8': 3,'gt8': 4}\n",
    "    age_map = {'below21': 0,'21': 1,'26': 2,'31': 3,'36': 4,'41': 5,'46': 6,'50plus': 7}\n",
    "    income_map = {'Less than $12500': 0,'$12500 - $24999': 1,'$25000 - $37499': 2,'$37500 - $49999': 3,\n",
    "    '$50000 - $62499': 4,'$62500 - $74999': 5,'$75000 - $87499': 6,'$87500 - $99999': 7,'$100000 or More': 8}\n",
    "    temperature_map = {30: 0,55: 1,80: 2}\n",
    "\n",
    "    # CoffeeHouse, CarryAway, RestaurantLessThan20, Restaurant20To50, Bar\n",
    "    df['CoffeeHouse'] = df['CoffeeHouse'].map(frequency_map)\n",
    "    df['CarryAway'] = df['CarryAway'].map(frequency_map)\n",
    "    df['RestaurantLessThan20'] = df['RestaurantLessThan20'].map(frequency_map)\n",
    "    df['Restaurant20To50'] = df['Restaurant20To50'].map(frequency_map)\n",
    "    df['Bar'] = df['Bar'].map(frequency_map)\n",
    "\n",
    "    #age\n",
    "    df['age'] = df['age'].map(age_map)\n",
    "\n",
    "    #income \n",
    "    df['income'] = df['income'].map(income_map)\n",
    "\n",
    "    #temperature\n",
    "    df['temperature'] = df['temperature'].map(temperature_map)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = value_mapping(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "12679    1\n",
       "12680    1\n",
       "12681    0\n",
       "12682    0\n",
       "12683    0\n",
       "Name: Y, Length: 12610, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Columns for ordinal encoding\n",
    "ordinal_cols = ['destination', 'weather', 'time', 'expiration', 'maritalStatus', 'education']\n",
    "\n",
    "# Define the specific ordering for ordinal columns\n",
    "ordinal_ordering = {\n",
    "    'destination': ['No Urgent Place', 'Home', 'Work'],\n",
    "    'weather': ['Sunny', 'Rainy', 'Snowy'],\n",
    "    'time': ['7AM', '10AM', '2PM', '6PM', '10PM'],\n",
    "    'expiration': ['2h', '1d'],\n",
    "    'maritalStatus': ['Single', 'Unmarried partner', 'Married partner', 'Divorced', 'Widowed'],\n",
    "    'education': ['Some High School', 'High School Graduate', 'Some college - no degree', \n",
    "                  'Associates degree', 'Bachelors degree', 'Graduate degree (Masters or Doctorate)']\n",
    "}\n",
    "\n",
    "# Ordinal encoding\n",
    "ordinal_encoder = OrdinalEncoder(categories=[ordinal_ordering[col] for col in ordinal_cols])\n",
    "df[ordinal_cols] = ordinal_encoder.fit_transform(df[ordinal_cols])\n",
    "\n",
    "# Identify columns to be one-hot encoded (excluding already binary or ordinal encoded columns)\n",
    "columns_to_encode = df.columns.drop(ordinal_cols + ['temperature', 'age', 'has_children', 'income',\n",
    "                                                    'Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20',\n",
    "                                                    'Restaurant20To50', 'toCoupon_GEQ15min', 'toCoupon_GEQ25min', \n",
    "                                                    'direction_same', 'Y'])  # Add any other columns to exclude\n",
    "\n",
    "# One-hot encoding for the non-binary categorical columns\n",
    "df = pd.get_dummies(df, columns=columns_to_encode, drop_first=False, dtype=int)\n",
    "\n",
    "# df now contains your processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Y\", axis=1)\n",
    "y = df[\"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "destination: [0. 1. 2.]\n",
      "weather: [0. 1. 2.]\n",
      "temperature: [1 2 0]\n",
      "time: [2. 1. 3. 0. 4.]\n",
      "expiration: [1. 0.]\n",
      "age: [1 6 2 3 5 7 4 0]\n",
      "maritalStatus: [1. 0. 2. 3. 4.]\n",
      "has_children: [1 0]\n",
      "education: [2. 4. 3. 1. 5. 0.]\n",
      "income: [3 5 1 6 4 2 8 7 0]\n",
      "Bar: [0 1 2 4 3]\n",
      "CoffeeHouse: [0 1 3 2 4]\n",
      "CarryAway: [2 3 4 1 0]\n",
      "RestaurantLessThan20: [3 2 1 4 0]\n",
      "Restaurant20To50: [2 1 0 4 3]\n",
      "toCoupon_GEQ15min: [0 1]\n",
      "toCoupon_GEQ25min: [0 1]\n",
      "direction_same: [0 1]\n",
      "passanger_Alone: [1 0]\n",
      "passanger_Friend(s): [0 1]\n",
      "passanger_Kid(s): [0 1]\n",
      "passanger_Partner: [0 1]\n",
      "coupon_Bar: [0 1]\n",
      "coupon_Carry out & Take away: [0 1]\n",
      "coupon_Coffee House: [0 1]\n",
      "coupon_Restaurant(20-50): [0 1]\n",
      "coupon_Restaurant(<20): [1 0]\n",
      "gender_Female: [1 0]\n",
      "gender_Male: [0 1]\n",
      "occupation_Architecture & Engineering: [0 1]\n",
      "occupation_Arts Design Entertainment Sports & Media: [0 1]\n",
      "occupation_Building & Grounds Cleaning & Maintenance: [0 1]\n",
      "occupation_Business & Financial: [0 1]\n",
      "occupation_Community & Social Services: [0 1]\n",
      "occupation_Computer & Mathematical: [0 1]\n",
      "occupation_Construction & Extraction: [0 1]\n",
      "occupation_Education&Training&Library: [0 1]\n",
      "occupation_Farming Fishing & Forestry: [0 1]\n",
      "occupation_Food Preparation & Serving Related: [0 1]\n",
      "occupation_Healthcare Practitioners & Technical: [0 1]\n",
      "occupation_Healthcare Support: [0 1]\n",
      "occupation_Installation Maintenance & Repair: [0 1]\n",
      "occupation_Legal: [0 1]\n",
      "occupation_Life Physical Social Science: [0 1]\n",
      "occupation_Management: [0 1]\n",
      "occupation_Office & Administrative Support: [0 1]\n",
      "occupation_Personal Care & Service: [0 1]\n",
      "occupation_Production Occupations: [0 1]\n",
      "occupation_Protective Service: [0 1]\n",
      "occupation_Retired: [0 1]\n",
      "occupation_Sales & Related: [0 1]\n",
      "occupation_Student: [0 1]\n",
      "occupation_Transportation & Material Moving: [0 1]\n",
      "occupation_Unemployed: [1 0]\n"
     ]
    }
   ],
   "source": [
    "for column in X.columns:\n",
    "    print(f\"{column}: {X[column].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce df to 10% of its original size randomly\n",
    "df = df.sample(frac=0.1, random_state=10)\n",
    "# save df to csv\n",
    "df.to_csv('my_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "#The silhouette score ranges from -1 to 1. A high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "def gower_silhouette_score(X, labels):\n",
    "    # Compute the Gower distance matrix\n",
    "    gower_dist_matrix = gower.gower_matrix(X)\n",
    "\n",
    "    # Initialize variables to store the intra and nearest-cluster distances\n",
    "    a = np.zeros(X.shape[0])\n",
    "    b = np.zeros(X.shape[0])\n",
    "\n",
    "    # Calculate the average intra-cluster distance (a) and the average nearest-cluster distance (b)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Same cluster mask\n",
    "        same_cluster_mask = (labels == labels[i])\n",
    "        \n",
    "        # Different cluster mask\n",
    "        different_cluster_mask = ~same_cluster_mask\n",
    "\n",
    "        # Intra-cluster distances (a)\n",
    "        a[i] = np.mean(gower_dist_matrix[i][same_cluster_mask])\n",
    "\n",
    "        # Nearest-cluster distances (b)\n",
    "        b[i] = np.min([np.mean(gower_dist_matrix[i][labels == label]) \n",
    "                       for label in set(labels) if label != labels[i]])\n",
    "\n",
    "    # Calculate the silhouette scores\n",
    "    s = (b - a) / np.maximum(a, b)\n",
    "\n",
    "    # Return the average silhouette score\n",
    "    return np.mean(s)\n",
    "\n",
    "# calculate dunn index for a clustering and gower distance matrix\n",
    "# The Dunn Index measures the ratio between the smallest distance between observations not in the same cluster to the largest intra-cluster distance.\n",
    "# A higher value indicates better clustering quality (i.e., clusters are compact and well-separated).\n",
    "def calculate_dunn_index(X, labels):\n",
    "    # Compute the Gower distance matrix\n",
    "    gower_dist_matrix = gower.gower_matrix(X)\n",
    "\n",
    "    # Initialize variables to store the intra and nearest-cluster distances\n",
    "    a = np.zeros(X.shape[0])\n",
    "    b = np.zeros(X.shape[0])\n",
    "\n",
    "    # Calculate the average intra-cluster distance (a) and the average nearest-cluster distance (b)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Same cluster mask\n",
    "        same_cluster_mask = (labels == labels[i])\n",
    "        \n",
    "        # Different cluster mask\n",
    "        different_cluster_mask = ~same_cluster_mask\n",
    "\n",
    "        # Intra-cluster distances (a)\n",
    "        a[i] = np.mean(gower_dist_matrix[i][same_cluster_mask])\n",
    "\n",
    "        # Nearest-cluster distances (b)\n",
    "        b[i] = np.min([np.mean(gower_dist_matrix[i][labels == label]) \n",
    "                       for label in set(labels) if label != labels[i]])\n",
    "\n",
    "    # Calculate the Dunn index\n",
    "    dunn_index = np.min(a) / np.max(b)\n",
    "\n",
    "    # Return the Dunn index\n",
    "    return dunn_index\n",
    "\n",
    "# calculate davies bouldin index for a clustering and gower distance matrix\n",
    "# Lower values are preferred, indicating that clusters are farther apart and less dispersed.\n",
    "def davies_bouldin_index(X, labels):\n",
    "    # Compute the Gower distance matrix\n",
    "    gower_dist_matrix = gower.gower_matrix(X)\n",
    "\n",
    "    # Initialize variables to store the intra and nearest-cluster distances\n",
    "    a = np.zeros(X.shape[0])\n",
    "    b = np.zeros(X.shape[0])\n",
    "\n",
    "    # Calculate the average intra-cluster distance (a) and the average nearest-cluster distance (b)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Same cluster mask\n",
    "        same_cluster_mask = (labels == labels[i])\n",
    "        \n",
    "        # Different cluster mask\n",
    "        different_cluster_mask = ~same_cluster_mask\n",
    "\n",
    "        # Intra-cluster distances (a)\n",
    "        a[i] = np.mean(gower_dist_matrix[i][same_cluster_mask])\n",
    "\n",
    "        # Nearest-cluster distances (b)\n",
    "        b[i] = np.min([np.mean(gower_dist_matrix[i][labels == label]) \n",
    "                       for label in set(labels) if label != labels[i]])\n",
    "\n",
    "    # Calculate the Davies-Bouldin index\n",
    "    db_index = np.mean((a + b) / np.max(b))\n",
    "\n",
    "    # Return the Davies-Bouldin index\n",
    "    return db_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Gower Silhouette Score: 0.15826708492325126\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Gower distance matrix\n",
    "gower_dist_matrix = gower.gower_matrix(X)\n",
    "\n",
    "# Convert the Gower distance matrix to a condensed distance matrix\n",
    "condensed_gower_dist_matrix = squareform(gower_dist_matrix)\n",
    "\n",
    "linkage_methods = ['single', 'complete', 'weighted', 'centroid', 'ward']\n",
    "n_clusters_range = range(2, 5)  # Example: considering 2 to 4 clusters\n",
    "\n",
    "for method in linkage_methods:\n",
    "    # Perform hierarchical clustering using the condensed Gower distance matrix\n",
    "    linked = linkage(condensed_gower_dist_matrix, method=method)\n",
    "    \n",
    "    for n_clusters in n_clusters_range:\n",
    "        # Obtain cluster labels\n",
    "        y_pred = fcluster(linked, n_clusters, criterion='maxclust') \n",
    "        # Calculate Silhouette Score\n",
    "        silhouette_avg = gower_silhouette_score(X, y_pred)\n",
    "        print(f\"Average Gower Silhouette Score: {silhouette_avg}\")\n",
    "        # Calculate Dunn Index\n",
    "        dunn_index_score = calculate_dunn_index(X, y_pred)\n",
    "        print(f\"Dunn Index: {dunn_index_score}\")\n",
    "        # Calculate Davies-Bouldin Index\n",
    "        db_index = davies_bouldin_index(X, y_pred)\n",
    "        print(f\"Davies-Bouldin Index: {db_index}\")\n",
    "\n",
    "        # Print contingency matrix and other metrics\n",
    "        print(f\"Metrics for {method.capitalize()} Linkage with {n_clusters} clusters:\")\n",
    "        print(f\"Contingency Matrix:\\n{metrics.cluster.contingency_matrix(y, y_pred)}\")\n",
    "        print(f\"Number of Clusters: {np.unique(y_pred).size}\")\n",
    "        print(f\"Samples per Cluster: {np.unique(y_pred, return_counts=True)[1]}\")\n",
    "\n",
    "        # Plot the dendrogram\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "        plt.title(f\"Dendrogram ({method.capitalize()} Linkage) with {n_clusters} Clusters\")\n",
    "        plt.axhline(y=n_clusters, color='r', linestyle='--')  # This line might need adjustment based on your analysis\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
